{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble script",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joVC7Kq16X91"
      },
      "source": [
        "Naive Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-aqJAc4IdIX",
        "outputId": "24b0d87d-29dd-4471-da34-09543434ecdb"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 44.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8dOdysw6WI5"
      },
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from transformers import BertTokenizerFast,TFBertModel\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrWGrimb5kEW"
      },
      "source": [
        "mode=\"validation\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZfrqV2S5nT2"
      },
      "source": [
        "if mode==\"validation\":\n",
        "  file_path=\"validation.csv\"\n",
        "  file_path_BIGRU=\"Spanbert_TC_BIGRU_validation.csv\"\n",
        "  file_path_conv=\"Spanbert_TC_Conv_validation.csv\"\n",
        "  file_path_tc=\"Spanbert_TC_validation.csv\"\n",
        "  final_file_path=f\"All_results_{mode}.csv\"\n",
        "else:\n",
        "  file_path=\"https://raw.github.com/ipavlopoulos/toxic_spans/master/data/tsd_test.csv\"\n",
        "  file_path_BIGRU=\"Spanbert_TC_BIGRU_test.csv\"\n",
        "  file_path_conv=\"Spanbert_TC_Conv_test.csv\"\n",
        "  file_path_tc=\"Spanbert_TC_test.csv\"\n",
        "  final_file_path=f\"All_results_{mode}.csv\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXdBz68V6Xn-"
      },
      "source": [
        "#test_data=https://raw.githubusercontent.com/nangokosu/Toxic_span_detection/main/spanBERT_TC/validation.csv?token=ANVHFMCFBYSQQPTBS5ATGOLASW6AK\n",
        "#validation_data=\"https://raw.githubusercontent.com/nangokosu/Toxic_span_detection/main/spanBERT_TC/validation.csv?token=ANVHFMCFBYSQQPTBS5ATGOLASW6AK\"\n",
        "df=pd.read_csv(file_path)[[\"spans\",\"text\"]]\n",
        "df[\"span\"]=df.spans.apply(lambda x: ast.literal_eval(x))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "t4M71TWj7bed",
        "outputId": "ceac00b8-f9b3-4817-d5d4-ab231e95b915"
      },
      "source": [
        "df_BIGRU=pd.read_csv(file_path_BIGRU)[[\"span\",\"text\"]]\n",
        "df_BIGRU.span=df_BIGRU.span.apply(lambda x: ast.literal_eval(x))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2b0afad198d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_BIGRU\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path_BIGRU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"span\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_BIGRU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_BIGRU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m     )\n\u001b[1;32m    439\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# TODO: fsspec can also handle HTTP via requests, but leaving this unchanged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CZC_4od7h8q"
      },
      "source": [
        "df_conv=pd.read_csv(file_path_conv)[[\"span\",\"text\"]]\n",
        "df_conv.span=df_conv.span.apply(lambda x: ast.literal_eval(x))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETamc1VDEDEt"
      },
      "source": [
        "df_tc=pd.read_csv(file_path_tc)[[\"span\",\"text\"]]\n",
        "df_tc.span=df_tc.span.apply(lambda x: ast.literal_eval(x))"
      ],
      "execution_count": 385,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS_Vz7w77qQj"
      },
      "source": [
        "df_methods=df_conv.merge(df_BIGRU,how=\"inner\",left_on=\"text\",right_on=\"text\")"
      ],
      "execution_count": 386,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foC9iBGM_Mzv"
      },
      "source": [
        "df_methods[\"correct_span\"]=df.span"
      ],
      "execution_count": 392,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy2eJTcN9AjG"
      },
      "source": [
        "def find_intersection(x,y):\n",
        "  set_1=set(x)\n",
        "  set_2=set(y)\n",
        "  intersection=set_1.intersection(set_2)\n",
        "  return list(intersection)"
      ],
      "execution_count": 387,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GQz3GuDDb7S"
      },
      "source": [
        "def find_union(x,y):\n",
        "  set_1=set(x)\n",
        "  set_2=set(y)\n",
        "  intersection=set_1.union(set_2)\n",
        "  return list(intersection)"
      ],
      "execution_count": 388,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jcmcwTw9OYn"
      },
      "source": [
        "df_methods[\"span_intersection\"]=df_methods[['span_x','span_y']].apply(lambda x: find_intersection(x[0],x[1]),axis=1)\n",
        "df_methods[\"span_union\"]=df_methods[['span_x','span_y']].apply(lambda x: find_union(x[0],x[1]),axis=1)\n",
        "df_methods[\"span_TC\"]=df_tc.span"
      ],
      "execution_count": 389,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZeKAMv9-_ap"
      },
      "source": [
        "def f1(predictions, gold):\n",
        "    \"\"\"\n",
        "    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n",
        "    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n",
        "    :param predictions: a list of predicted offsets\n",
        "    :param gold: a list of offsets serving as the ground truth\n",
        "    :return: a score between 0 and 1\n",
        "    \"\"\"\n",
        "    if len(gold) == 0:\n",
        "        return 1. if len(predictions) == 0 else 0.\n",
        "    if len(predictions) == 0:\n",
        "        return 0.\n",
        "    predictions_set = set(predictions)\n",
        "    gold_set = set(gold)\n",
        "    nom = 2 * len(predictions_set.intersection(gold_set))\n",
        "    denom = len(predictions_set) + len(gold_set)\n",
        "    return float(nom)/float(denom)"
      ],
      "execution_count": 395,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV-sEf8-A3-Q"
      },
      "source": [
        "df_methods[\"f1_intersection\"]=df_methods[[\"span_intersection\",\"correct_span\"]].apply(lambda x: f1(x[0],x[1]),axis=1)\n",
        "df_methods[\"f1_conv\"]=df_methods[[\"span_x\",\"correct_span\"]].apply(lambda x: f1(x[0],x[1]),axis=1)\n",
        "df_methods[\"f1_BIGRU\"]=df_methods[[\"span_y\",\"correct_span\"]].apply(lambda x: f1(x[0],x[1]),axis=1)\n",
        "df_methods[\"f1_union\"]=df_methods[[\"span_union\",\"correct_span\"]].apply(lambda x: f1(x[0],x[1]),axis=1)\n",
        "df_methods[\"f1_TC\"]=df_methods[[\"span_TC\",\"correct_span\"]].apply(lambda x: f1(x[0],x[1]),axis=1)"
      ],
      "execution_count": 396,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efKGckCCCBbT",
        "outputId": "13a16a09-66c6-447e-c18f-2031aa23d47f"
      },
      "source": [
        "print(df_methods[\"f1_intersection\"].mean())\n",
        "print(df_methods[\"f1_conv\"].mean())\n",
        "print(df_methods[\"f1_BIGRU\"].mean())\n",
        "print(df_methods[\"f1_union\"].mean())\n",
        "print(df_methods[\"f1_TC\"].mean())"
      ],
      "execution_count": 397,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6150828393462038\n",
            "0.6609502687231014\n",
            "0.6498513353348693\n",
            "0.6879483408143138\n",
            "0.6839899478504031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlskh15GIMuD"
      },
      "source": [
        "def find_spans(span_index,text):\n",
        "  '''Function takes in the span index list and returns the text those indices correspond to'''\n",
        "  try:  \n",
        "    span_index=[int(item) for item in span_index]\n",
        "    current_index=span_index[0]\n",
        "    spans=[]\n",
        "    span_text=[]\n",
        "    for i in range(len(span_index)-1):\n",
        "      if span_index[i+1]-span_index[i]>1:\n",
        "        spans.append((current_index,span_index[i]+1))\n",
        "        current_index=span_index[i+1]\n",
        "    if current_index==span_index[0]:\n",
        "      spans.append((current_index,span_index[-1]+1))\n",
        "    return spans\n",
        "  except:\n",
        "    return []"
      ],
      "execution_count": 398,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7w-0oauITQu"
      },
      "source": [
        "spanbert_path='/content/drive/MyDrive/Span NN materials/spanbert_hf_base/'"
      ],
      "execution_count": 399,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHBobkWXIW5q"
      },
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(spanbert_path)"
      ],
      "execution_count": 400,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KwI3IXwGZZm"
      },
      "source": [
        "class TweetResults:\n",
        "  def __init__(self,text,span_index,max_len,doc_max_len):\n",
        "    self.text=text\n",
        "    self.span_index=span_index\n",
        "    self.max_len=max_len\n",
        "    self.doc_max_len=doc_max_len\n",
        "  \n",
        "  def process(self):\n",
        "    text=self.text\n",
        "    span_index=self.span_index\n",
        "    character_in_ans=[0]*len(text)\n",
        "    spans=find_spans(span_index,text)\n",
        "    for start,end in spans: # marking the 1s\n",
        "      for idx in range(start,end):\n",
        "        character_in_ans[idx]=1\n",
        "\n",
        "    # Generates tokenized text\n",
        "    tokenized_text = tokenizer.encode_plus(text, return_offsets_mapping=True, max_length = self.max_len)\n",
        "\n",
        "    # Generate the actual tokens:\n",
        "    ans_token_idx = []\n",
        "    for idx, (start, end) in enumerate(tokenized_text.offset_mapping):\n",
        "        if sum(character_in_ans[start:end]) > 0:\n",
        "            ans_token_idx.append([0,1])\n",
        "        else: \n",
        "            ans_token_idx.append([1,0])\n",
        "\n",
        "    # Padding\n",
        "    padding_length_toxic=self.doc_max_len - len(ans_token_idx)\n",
        "    if padding_length_toxic>0:\n",
        "      #character_in_ans=character_in_ans+[0]*padding_length_toxic\n",
        "      ans_token_idx=ans_token_idx+[[0,0]]*padding_length_toxic\n",
        "    \n",
        "    self.ans_token_idx=ans_token_idx\n",
        "    self.context_token_to_char = tokenized_text.offset_mapping"
      ],
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqvJgiW4J1up"
      },
      "source": [
        "def create_tweet_examples(data,max_len,doc_max_len):\n",
        "    tweet_examples = []\n",
        "    count = 0\n",
        "    for index, row in data.iterrows():\n",
        "        span_index = row['span']\n",
        "        text=row['text']\n",
        "        ## turn the data into TrainTweetExample objects\n",
        "        try:\n",
        "          tweet_eg = TweetResults(text, span_index,max_len,doc_max_len)\n",
        "          tweet_eg.process()\n",
        "          tweet_examples.append(tweet_eg)\n",
        "        except: ## keep track of the number that can't be tokenized/processed\n",
        "            count += 1 \n",
        "    print(\"Couldn't process\",count,\"points\")\n",
        "    return tweet_examples"
      ],
      "execution_count": 401,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD9jlOixNDGQ"
      },
      "source": [
        "max_len=400\n",
        "doc_max_len=400"
      ],
      "execution_count": 402,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn8Btsr8NLrM",
        "outputId": "b1c67a22-775d-41c7-f00e-c1de0803320e"
      },
      "source": [
        "BIGRU_data=create_tweet_examples(validation_BIGRU,max_len,doc_max_len)\n",
        "conv_data=create_tweet_examples(validation_conv,max_len,doc_max_len)\n",
        "correct_data=create_tweet_examples(validation,max_len,doc_max_len)"
      ],
      "execution_count": 403,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Couldn't process 0 points\n",
            "Couldn't process 0 points\n",
            "Couldn't process 0 points\n",
            "Couldn't process 0 points\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWKzn9DNNpbk"
      },
      "source": [
        "import numpy as np\n",
        "BIGRU_data_prediction=np.array([element.ans_token_idx for element in BIGRU_data])[:,:,1].reshape(-1,400,1)\n",
        "conv_data_prediction=np.array([element.ans_token_idx for element in conv_data])[:,:,1].reshape(-1,400,1)\n",
        "correct_data_prediction=np.array([element.ans_token_idx for element in correct_data])"
      ],
      "execution_count": 404,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve-xXeDpObKG"
      },
      "source": [
        "def create_model(max_len,doc_max_len):\n",
        "  input_BIGRU=layers.Input(shape=(400,1))\n",
        "  input_conv=layers.Input(shape=(400,1))\n",
        "  input_tc=layers.Input(shape=(400,1))\n",
        "  concat=layers.Concatenate()([input_BIGRU,input_conv,input_tc])\n",
        "  #concat=layers.TimeDistributed(layers.Dense(1,activation=\"relu\"))(concat)\n",
        "  concat=layers.TimeDistributed(layers.Dense(2,activation=\"softmax\"))(concat)\n",
        "  return Model([input_BIGRU,input_conv,input_tc],concat)"
      ],
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIHJStywSx-u"
      },
      "source": [
        "model=create_model(max_len,doc_max_len)"
      ],
      "execution_count": 405,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiPmdwgmXjNw",
        "outputId": "3c92d2a5-3927-4f81-8db5-d783d86a43bb"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 406,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_26\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_60 (InputLayer)           [(None, 400, 1)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_61 (InputLayer)           [(None, 400, 1)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_62 (InputLayer)           [(None, 400, 1)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 400, 3)       0           input_60[0][0]                   \n",
            "                                                                 input_61[0][0]                   \n",
            "                                                                 input_62[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_24 (TimeDistri (None, 400, 2)       8           concatenate_28[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 8\n",
            "Trainable params: 8\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROCAry4eVkom"
      },
      "source": [
        "early_stop=EarlyStopping(monitor=\"val_accuracy\",patience=10,restore_best_weights=True)\n",
        "model_check=ModelCheckpoint(\"Ensemble_model\",monitor=\"val_accuracy\",save_best_only=True)"
      ],
      "execution_count": 407,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwTuItCTUtCk"
      },
      "source": [
        "model.compile(optimizer=\"Adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
      ],
      "execution_count": 408,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtRikjjlUgqi",
        "outputId": "a463e754-779c-4a13-cc3a-776da5506f7e"
      },
      "source": [
        "model.fit([BIGRU_data_prediction,conv_data_prediction,tc_data_prediction],correct_data_prediction,epochs=200,validation_split=0.10,callbacks=[early_stop,model_check])"
      ],
      "execution_count": 409,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "23/23 [==============================] - 1s 18ms/step - loss: 0.0889 - accuracy: 0.9881 - val_loss: 0.0990 - val_accuracy: 0.9892\n",
            "Epoch 2/200\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0875 - accuracy: 0.9887 - val_loss: 0.0962 - val_accuracy: 0.9892\n",
            "Epoch 3/200\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0826 - accuracy: 0.9912 - val_loss: 0.0936 - val_accuracy: 0.9892\n",
            "Epoch 4/200\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.0874 - accuracy: 0.9896 - val_loss: 0.0911 - val_accuracy: 0.9892\n",
            "Epoch 5/200\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.0815 - accuracy: 0.9907 - val_loss: 0.0887 - val_accuracy: 0.9892\n",
            "Epoch 6/200\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.0824 - accuracy: 0.9895 - val_loss: 0.0864 - val_accuracy: 0.9892\n",
            "Epoch 7/200\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.0741 - accuracy: 0.9910 - val_loss: 0.0841 - val_accuracy: 0.9892\n",
            "Epoch 8/200\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0669 - accuracy: 0.9911 - val_loss: 0.0821 - val_accuracy: 0.9892\n",
            "Epoch 9/200\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.0689 - accuracy: 0.9925 - val_loss: 0.0800 - val_accuracy: 0.9892\n",
            "Epoch 10/200\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.0665 - accuracy: 0.9902 - val_loss: 0.0782 - val_accuracy: 0.9892\n",
            "Epoch 11/200\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.0667 - accuracy: 0.9910 - val_loss: 0.0763 - val_accuracy: 0.9892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f003ff95050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 409
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0_xDyCXYu-Z"
      },
      "source": [
        "def get_indices(x):\n",
        "  indice_prediction=[]\n",
        "  for single_prediction in x:\n",
        "    indice_prediction.append([idx for idx,value in enumerate(single_prediction) if value[1]>=0.5])\n",
        "  return indice_prediction"
      ],
      "execution_count": 410,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lVgprrobtzY"
      },
      "source": [
        "def get_indices_final(indice_prediction,tweet_examples):\n",
        "  all_correct_indices=[]\n",
        "  for i in range(len(indice_prediction)):\n",
        "    correct_indices=[]\n",
        "    indices=indice_prediction[i]\n",
        "    tweet=tweet_examples[i].context_token_to_char\n",
        "    #assert len(indices)>len(tweet)\n",
        "    for i in indices:\n",
        "      if i <len(tweet):\n",
        "        start,end=tweet[i]\n",
        "        correct_indices.extend([index for index in range(start,end)])\n",
        "    all_correct_indices.append(correct_indices)\n",
        "  return all_correct_indices"
      ],
      "execution_count": 411,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QlJl2sPbw8W"
      },
      "source": [
        "def all_f1_scores(predictions,golds):\n",
        "  f1_scores=[]\n",
        "  for i in range(len(predictions)):\n",
        "    f1_score=f1(predictions[i],golds.span.values[i])\n",
        "    f1_scores.append(f1_score)\n",
        "  return f1_scores"
      ],
      "execution_count": 412,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzwb3uBNcX0D"
      },
      "source": [
        "def convert_to_indices(input_data,tweet_examples):\n",
        "  '''\n",
        "  Input data takes the form of a list of two arrays.\n",
        "  '''\n",
        "  global model\n",
        "  predictions=model.predict(input_data)\n",
        "  indice_prediction=get_indices(predictions)\n",
        "  final_indices=get_indices_final(indice_prediction,tweet_examples)\n",
        "  return final_indices"
      ],
      "execution_count": 413,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEP2YSthb83y"
      },
      "source": [
        "indices_val=convert_to_indices([BIGRU_data_prediction,conv_data_prediction],BIGRU_data)\n",
        "df_methods[\"f1_ensemble\"]=all_f1_scores(indices_val,validation)"
      ],
      "execution_count": 414,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VO_xY9tdKSb",
        "outputId": "ab495823-813e-4acf-e5cc-d1f7cd046770"
      },
      "source": [
        "df_methods.to_csv(final_file_path,index=False)"
      ],
      "execution_count": 416,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6612158252671426"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 416
        }
      ]
    }
  ]
}