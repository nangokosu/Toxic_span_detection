{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble script",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joVC7Kq16X91"
      },
      "source": [
        "Naive Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-aqJAc4IdIX",
        "outputId": "d9611ed5-6ee2-40fc-aedc-b9b098a2a00d"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8dOdysw6WI5"
      },
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from transformers import BertTokenizerFast,TFBertModel\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHzJ4iXTeM65"
      },
      "source": [
        "root_path=\"\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrWGrimb5kEW"
      },
      "source": [
        "mode=\"test\""
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZfrqV2S5nT2"
      },
      "source": [
        "if mode==\"validation\":\n",
        "  file_path=root_path+\"validation.csv\"\n",
        "  file_path_BIGRU=root_path+\"Spanbert_TC_BIGRU_validation.csv\"\n",
        "  file_path_conv=root_path+\"Spanbert_TC_Conv_validation.csv\"\n",
        "  file_path_tc=root_path+\"Spanbert_TC_validation.csv\"\n",
        "  final_file_path=root_path+f\"All_results_{mode}.csv\"\n",
        "else:\n",
        "  file_path=\"https://raw.github.com/ipavlopoulos/toxic_spans/master/data/tsd_test.csv\"\n",
        "  file_path_BIGRU=root_path+\"Spanbert_TC_BIGRU_test.csv\"\n",
        "  file_path_conv=root_path+\"Spanbert_TC_Conv_test.csv\"\n",
        "  file_path_tc=root_path+\"Spanbert_TC_test.csv\"\n",
        "  final_file_path=root_path+f\"All_results_{mode}.csv\""
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXdBz68V6Xn-"
      },
      "source": [
        "#test_data=https://raw.githubusercontent.com/nangokosu/Toxic_span_detection/main/spanBERT_TC/validation.csv?token=ANVHFMCFBYSQQPTBS5ATGOLASW6AK\n",
        "#validation_data=\"https://raw.githubusercontent.com/nangokosu/Toxic_span_detection/main/spanBERT_TC/validation.csv?token=ANVHFMCFBYSQQPTBS5ATGOLASW6AK\"\n",
        "df=pd.read_csv(file_path)[[\"spans\",\"text\"]]\n",
        "df[\"span\"]=df.spans.apply(lambda x: ast.literal_eval(x))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4M71TWj7bed"
      },
      "source": [
        "df_BIGRU=pd.read_csv(file_path_BIGRU)[[\"span\",\"text\"]]\n",
        "df_BIGRU.span=df_BIGRU.span.apply(lambda x: ast.literal_eval(x))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CZC_4od7h8q"
      },
      "source": [
        "df_conv=pd.read_csv(file_path_conv)[[\"span\",\"text\"]]\n",
        "df_conv.span=df_conv.span.apply(lambda x: ast.literal_eval(x))"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETamc1VDEDEt"
      },
      "source": [
        "df_tc=pd.read_csv(file_path_tc)[[\"span\",\"text\"]]\n",
        "df_tc.span=df_tc.span.apply(lambda x: ast.literal_eval(x))"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS_Vz7w77qQj"
      },
      "source": [
        "df_methods=df_conv.merge(df_BIGRU,how=\"inner\",left_on=\"text\",right_on=\"text\")"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foC9iBGM_Mzv"
      },
      "source": [
        "df_methods[\"correct_span\"]=df.span"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy2eJTcN9AjG"
      },
      "source": [
        "def find_intersection(x,y):\n",
        "  set_1=set(x)\n",
        "  set_2=set(y)\n",
        "  intersection=set_1.intersection(set_2)\n",
        "  return list(intersection)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GQz3GuDDb7S"
      },
      "source": [
        "def find_union(x,y):\n",
        "  set_1=set(x)\n",
        "  set_2=set(y)\n",
        "  intersection=set_1.union(set_2)\n",
        "  return list(intersection)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jcmcwTw9OYn"
      },
      "source": [
        "df_methods[\"span_intersection\"]=df_methods[['span_x','span_y']].apply(lambda x: find_intersection(x[0],x[1]),axis=1)\n",
        "df_methods[\"span_union\"]=df_methods[['span_x','span_y']].apply(lambda x: find_union(x[0],x[1]),axis=1)\n",
        "df_methods[\"span_TC\"]=df_tc.span"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZeKAMv9-_ap"
      },
      "source": [
        "def f1(predictions, gold):\n",
        "    \"\"\"\n",
        "    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n",
        "    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n",
        "    :param predictions: a list of predicted offsets\n",
        "    :param gold: a list of offsets serving as the ground truth\n",
        "    :return: a score between 0 and 1\n",
        "    \"\"\"\n",
        "    if len(gold) == 0:\n",
        "        return 1. if len(predictions) == 0 else 0.\n",
        "    if len(predictions) == 0:\n",
        "        return 0.\n",
        "    predictions_set = set(predictions)\n",
        "    gold_set = set(gold)\n",
        "    nom = 2 * len(predictions_set.intersection(gold_set))\n",
        "    denom = len(predictions_set) + len(gold_set)\n",
        "    return float(nom)/float(denom)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV-sEf8-A3-Q"
      },
      "source": [
        "df_methods[\"f1_intersection\"]=df_methods[[\"span_intersection\",\"correct_span\"]].apply(lambda x: f1(x[0],x[1]),axis=1)\n",
        "df_methods[\"f1_conv\"]=df_methods[[\"span_x\",\"correct_span\"]].apply(lambda x: f1(x[0],x[1]),axis=1)\n",
        "df_methods[\"f1_BIGRU\"]=df_methods[[\"span_y\",\"correct_span\"]].apply(lambda x: f1(x[0],x[1]),axis=1)\n",
        "df_methods[\"f1_union\"]=df_methods[[\"span_union\",\"correct_span\"]].apply(lambda x: f1(x[0],x[1]),axis=1)\n",
        "df_methods[\"f1_TC\"]=df_methods[[\"span_TC\",\"correct_span\"]].apply(lambda x: f1(x[0],x[1]),axis=1)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efKGckCCCBbT",
        "outputId": "4bb9941e-1d9f-4061-dea2-0e84560dfcd6"
      },
      "source": [
        "print(df_methods[\"f1_intersection\"].mean())\n",
        "print(df_methods[\"f1_conv\"].mean())\n",
        "print(df_methods[\"f1_BIGRU\"].mean())\n",
        "print(df_methods[\"f1_union\"].mean())\n",
        "print(df_methods[\"f1_TC\"].mean())"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7854275734207472\n",
            "0.8090226837052209\n",
            "0.8184481137819403\n",
            "0.8330105925622171\n",
            "0.8424839311239191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlskh15GIMuD"
      },
      "source": [
        "def find_spans(span_index,text):\n",
        "  '''Function takes in the span index list and returns the text those indices correspond to'''\n",
        "  try:  \n",
        "    span_index=[int(item) for item in span_index]\n",
        "    current_index=span_index[0]\n",
        "    spans=[]\n",
        "    span_text=[]\n",
        "    for i in range(len(span_index)-1):\n",
        "      if span_index[i+1]-span_index[i]>1:\n",
        "        spans.append((current_index,span_index[i]+1))\n",
        "        current_index=span_index[i+1]\n",
        "    if current_index==span_index[0]:\n",
        "      spans.append((current_index,span_index[-1]+1))\n",
        "    return spans\n",
        "  except:\n",
        "    return []"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7w-0oauITQu"
      },
      "source": [
        "spanbert_path='/content/drive/MyDrive/Span NN materials/spanbert_hf_base/'"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHBobkWXIW5q"
      },
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(spanbert_path)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KwI3IXwGZZm"
      },
      "source": [
        "class TweetResults:\n",
        "  def __init__(self,text,span_index,max_len,doc_max_len):\n",
        "    self.text=text\n",
        "    self.span_index=span_index\n",
        "    self.max_len=max_len\n",
        "    self.doc_max_len=doc_max_len\n",
        "  \n",
        "  def process(self):\n",
        "    text=self.text\n",
        "    span_index=self.span_index\n",
        "    character_in_ans=[0]*len(text)\n",
        "    spans=find_spans(span_index,text)\n",
        "    for start,end in spans: # marking the 1s\n",
        "      for idx in range(start,end):\n",
        "        character_in_ans[idx]=1\n",
        "\n",
        "    # Generates tokenized text\n",
        "    tokenized_text = tokenizer.encode_plus(text, return_offsets_mapping=True, max_length = self.max_len)\n",
        "\n",
        "    # Generate the actual tokens:\n",
        "    ans_token_idx = []\n",
        "    for idx, (start, end) in enumerate(tokenized_text.offset_mapping):\n",
        "        if sum(character_in_ans[start:end]) > 0:\n",
        "            ans_token_idx.append([0,1])\n",
        "        else: \n",
        "            ans_token_idx.append([1,0])\n",
        "\n",
        "    # Padding\n",
        "    padding_length_toxic=self.doc_max_len - len(ans_token_idx)\n",
        "    if padding_length_toxic>0:\n",
        "      #character_in_ans=character_in_ans+[0]*padding_length_toxic\n",
        "      ans_token_idx=ans_token_idx+[[0,0]]*padding_length_toxic\n",
        "    \n",
        "    self.ans_token_idx=ans_token_idx\n",
        "    self.context_token_to_char = tokenized_text.offset_mapping"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqvJgiW4J1up"
      },
      "source": [
        "def create_tweet_examples(data,max_len,doc_max_len):\n",
        "    tweet_examples = []\n",
        "    count = 0\n",
        "    for index, row in data.iterrows():\n",
        "        span_index = row['span']\n",
        "        text=row['text']\n",
        "        ## turn the data into TrainTweetExample objects\n",
        "        try:\n",
        "          tweet_eg = TweetResults(text, span_index,max_len,doc_max_len)\n",
        "          tweet_eg.process()\n",
        "          tweet_examples.append(tweet_eg)\n",
        "        except: ## keep track of the number that can't be tokenized/processed\n",
        "            count += 1 \n",
        "    print(\"Couldn't process\",count,\"points\")\n",
        "    return tweet_examples"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD9jlOixNDGQ"
      },
      "source": [
        "max_len=400\n",
        "doc_max_len=400"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn8Btsr8NLrM",
        "outputId": "4808c389-39ca-49a6-cf18-462de551f65d"
      },
      "source": [
        "BIGRU_data=create_tweet_examples(df_BIGRU,max_len,doc_max_len)\n",
        "conv_data=create_tweet_examples(df_conv,max_len,doc_max_len)\n",
        "correct_data=create_tweet_examples(df,max_len,doc_max_len)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Couldn't process 0 points\n",
            "Couldn't process 0 points\n",
            "Couldn't process 0 points\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWKzn9DNNpbk"
      },
      "source": [
        "import numpy as np\n",
        "BIGRU_data_prediction=np.array([element.ans_token_idx for element in BIGRU_data])[:,:,1].reshape(-1,400,1)\n",
        "conv_data_prediction=np.array([element.ans_token_idx for element in conv_data])[:,:,1].reshape(-1,400,1)\n",
        "correct_data_prediction=np.array([element.ans_token_idx for element in correct_data])"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve-xXeDpObKG"
      },
      "source": [
        "def create_model(max_len,doc_max_len):\n",
        "  input_BIGRU=layers.Input(shape=(400,1))\n",
        "  input_conv=layers.Input(shape=(400,1))\n",
        "  #input_tc=layers.Input(shape=(400,1))\n",
        "  concat=layers.Concatenate()([input_BIGRU,input_conv])\n",
        "  #concat=layers.TimeDistributed(layers.Dense(1,activation=\"relu\"))(concat)\n",
        "  concat=layers.TimeDistributed(layers.Dense(2,activation=\"softmax\"))(concat)\n",
        "  return Model([input_BIGRU,input_conv],concat)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIHJStywSx-u"
      },
      "source": [
        "model=create_model(max_len,doc_max_len)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiPmdwgmXjNw",
        "outputId": "c96d4d64-1b90-4c8a-ece0-75924d162824"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 400, 1)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_9 (InputLayer)            [(None, 400, 1)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 400, 2)       0           input_8[0][0]                    \n",
            "                                                                 input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 400, 2)       6           concatenate_3[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 6\n",
            "Trainable params: 6\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROCAry4eVkom"
      },
      "source": [
        "early_stop=EarlyStopping(monitor=\"val_accuracy\",patience=10,restore_best_weights=True)\n",
        "model_check=ModelCheckpoint(root_path+\"Ensemble_model\",monitor=\"val_accuracy\",save_best_only=True)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwTuItCTUtCk"
      },
      "source": [
        "model.compile(optimizer=\"Adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtRikjjlUgqi",
        "outputId": "d690a28c-140d-4c12-9f8c-540712b2d597"
      },
      "source": [
        "model.fit([BIGRU_data_prediction,conv_data_prediction],correct_data_prediction,epochs=200,validation_split=0.10,callbacks=[early_stop,model_check])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "23/23 [==============================] - 1s 17ms/step - loss: 0.0864 - accuracy: 0.9928 - val_loss: 0.0987 - val_accuracy: 0.9893\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Span NN materials/Ensemble_model/assets\n",
            "Epoch 2/200\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.0852 - accuracy: 0.9900 - val_loss: 0.0961 - val_accuracy: 0.9890\n",
            "Epoch 3/200\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.0878 - accuracy: 0.9904 - val_loss: 0.0936 - val_accuracy: 0.9890\n",
            "Epoch 4/200\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.0786 - accuracy: 0.9912 - val_loss: 0.0912 - val_accuracy: 0.9890\n",
            "Epoch 5/200\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.0795 - accuracy: 0.9894 - val_loss: 0.0889 - val_accuracy: 0.9890\n",
            "Epoch 6/200\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.0768 - accuracy: 0.9905 - val_loss: 0.0867 - val_accuracy: 0.9890\n",
            "Epoch 7/200\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.0746 - accuracy: 0.9926 - val_loss: 0.0845 - val_accuracy: 0.9890\n",
            "Epoch 8/200\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.0726 - accuracy: 0.9911 - val_loss: 0.0825 - val_accuracy: 0.9890\n",
            "Epoch 9/200\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.0723 - accuracy: 0.9918 - val_loss: 0.0805 - val_accuracy: 0.9890\n",
            "Epoch 10/200\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.0681 - accuracy: 0.9916 - val_loss: 0.0786 - val_accuracy: 0.9890\n",
            "Epoch 11/200\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.0690 - accuracy: 0.9913 - val_loss: 0.0768 - val_accuracy: 0.9890\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2174561b10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0_xDyCXYu-Z"
      },
      "source": [
        "def get_indices(x):\n",
        "  indice_prediction=[]\n",
        "  for single_prediction in x:\n",
        "    indice_prediction.append([idx for idx,value in enumerate(single_prediction) if value[1]>=0.5])\n",
        "  return indice_prediction"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lVgprrobtzY"
      },
      "source": [
        "def get_indices_final(indice_prediction,tweet_examples):\n",
        "  all_correct_indices=[]\n",
        "  for i in range(len(indice_prediction)):\n",
        "    correct_indices=[]\n",
        "    indices=indice_prediction[i]\n",
        "    tweet=tweet_examples[i].context_token_to_char\n",
        "    #assert len(indices)>len(tweet)\n",
        "    for i in indices:\n",
        "      if i <len(tweet):\n",
        "        start,end=tweet[i]\n",
        "        correct_indices.extend([index for index in range(start,end)])\n",
        "    all_correct_indices.append(correct_indices)\n",
        "  return all_correct_indices"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QlJl2sPbw8W"
      },
      "source": [
        "def all_f1_scores(predictions,golds):\n",
        "  f1_scores=[]\n",
        "  for i in range(len(predictions)):\n",
        "    f1_score=f1(predictions[i],golds.span.values[i])\n",
        "    f1_scores.append(f1_score)\n",
        "  return f1_scores"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzwb3uBNcX0D"
      },
      "source": [
        "def convert_to_indices(input_data,tweet_examples):\n",
        "  '''\n",
        "  Input data takes the form of a list of two arrays.\n",
        "  '''\n",
        "  global model\n",
        "  predictions=model.predict(input_data)\n",
        "  indice_prediction=get_indices(predictions)\n",
        "  final_indices=get_indices_final(indice_prediction,tweet_examples)\n",
        "  return final_indices"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEP2YSthb83y"
      },
      "source": [
        "indices_pred=convert_to_indices([BIGRU_data_prediction,conv_data_prediction],BIGRU_data)\n",
        "df_methods[\"f1_ensemble\"]=all_f1_scores(indices_pred,df)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiCFBifafkIk",
        "outputId": "189c890e-74ce-4745-d8dd-19f43218d105"
      },
      "source": [
        "print(np.mean(df_methods[\"f1_ensemble\"]))\n",
        "print(np.mean(df_methods[\"f1_TC\"]))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8088108866903423\n",
            "0.8424839311239191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VO_xY9tdKSb"
      },
      "source": [
        "df_methods.to_csv(final_file_path,index=False)"
      ],
      "execution_count": 96,
      "outputs": []
    }
  ]
}